{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model.data \n",
    "본 노트에서는 `torch.utils.data.Dataset` class를 subclassing하여 custom Dataset인 `Corpus` class를 구현하고, 이와 함께`torch.utils.data.DataLoader` class를 이용하여 딥러닝 모형을 위한 data pipeline을 구현해보도록 합니다. 이 때, `build_vocab.ipynb` 노트에서 활용했었던 `Vocab` class, `Tokenizer` class, `PadSequence` class를 이용하여 전처리 과정을 data pipeline에 통합하도록 합니다. 구현한 `Corpus` class는 모듈화시 data pipeline에 관련된 script에 존재합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Callable, List, Tuple\n",
    "from pprint import pprint\n",
    "from mecab import MeCab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model.utils import Vocab, Tokenizer, PadSequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Corpus` class\n",
    "`Corpus` class는 PyTorch official tutorial 중 하나인 [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#data-loading-and-processing-tutorial)을 따라하면 쉽게 구현할 수 있습니다. 마찬가지로 `__init__`, `__len__`, `__getitem__`을 method overriding으로 구현하도록 합니다. `__getitem__`을 구현할 때, 필요한 전처리를 안쪽에 구현하는 것이 포인트입니다. `Corpus` class가 parameter로 전처리에 관련된 함수를 전달받아서 이를 `__getitem__`에서 활용하도록 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus(Dataset):\n",
    "    \"\"\"Corpus class\"\"\"\n",
    "    def __init__(self, filepath: str, transform_fn: Callable[[str], List[int]]) -> None:\n",
    "        \"\"\"Instantiating Corpus class\n",
    "\n",
    "        Args:\n",
    "            filepath (str): filepath\n",
    "            transform_fn (Callable): a function that can act as a transformer\n",
    "        \"\"\"\n",
    "        self._corpus = pd.read_csv(filepath, sep='\\t').loc[:, ['document', 'label']]\n",
    "        self._transform = transform_fn\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._corpus)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        tokens2indices = torch.tensor(self._transform(self._corpus.iloc[idx]['document']))\n",
    "        label = torch.tensor(self._corpus.iloc[idx]['label'])\n",
    "        return tokens2indices, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 구현하면 `Corpus` class의 `transform_fn` argument에 전처리에 관련된 함수를 parameter로 전달함으로써 매우 쉽게 data pipeline의 일부분을 구현할 수 있습니다. 이를 위해서 아래의 단계를 진행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `build_vocab.ipynb` 노트에서 생성했던 `Vocab` class의 instance를 `split_fn`으로 활용했던 `Mecab` instance의 `morphs` 함수를 불러오고, `PadSequence`의 instance를 생성합니다. 이때 padding의 길이는 `eda.ipynb` 노트에서 탐색했던 결과로 padding합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path.cwd() / 'data'\n",
    "list_of_dataset = list(data_dir.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('/root/Documents/archive/strnlp/exercise/data/.DS_Store'),\n",
      " PosixPath('/root/Documents/archive/strnlp/exercise/data/morphs_eojeol.pkl'),\n",
      " PosixPath('/root/Documents/archive/strnlp/exercise/data/train.txt'),\n",
      " PosixPath('/root/Documents/archive/strnlp/exercise/data/morphs_vec.pkl'),\n",
      " PosixPath('/root/Documents/archive/strnlp/exercise/data/validation.txt'),\n",
      " PosixPath('/root/Documents/archive/strnlp/exercise/data/tokenizer.pkl'),\n",
      " PosixPath('/root/Documents/archive/strnlp/exercise/data/test.txt'),\n",
      " PosixPath('/root/Documents/archive/strnlp/exercise/data/vocab.pkl')]\n"
     ]
    }
   ],
   "source": [
    "pprint(list_of_dataset)\n",
    "with open(list_of_dataset[-1], mode='rb') as io:\n",
    "    vocab = pickle.load(io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장을 어절기준으로 보는 split_fn을 작성\n",
    "def split_eojeol(s: str) -> List[str]:\n",
    "    return s.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_sequence = PadSequence(length=32, pad_val=vocab.to_indices(vocab.padding_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Tokenizer` class의  instance를 생성할 때, `vocab`, `split_morphs`, `pad_sequence`를 parameter로 전달하여 instance를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab=vocab, split_fn=split_eojeol, pad_fn=pad_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Corpus` class의 instance를 생성할 때, `transform_fn` argument에 `tokenizer.split_and_transform`  멤버함수를 parameter로 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_corpus = Corpus(list_of_dataset[2], transform_fn=tokenizer.split_and_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>애들 욕하지마라 지들은 뭐 그렇게 잘났나? 솔까 거기 나오는 귀여운 애들이 당신들보...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>여전히 반복되고 있는 80년대 한국 멜로 영화의 유치함.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>쉐임리스 스티브와 피오나가 손오공 부르마로 ㅋㅋㅋ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0점은 없나요?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>제발 시즌2 ㅜㅜ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            document  label\n",
       "0  애들 욕하지마라 지들은 뭐 그렇게 잘났나? 솔까 거기 나오는 귀여운 애들이 당신들보...      1\n",
       "1                    여전히 반복되고 있는 80년대 한국 멜로 영화의 유치함.      0\n",
       "2                        쉐임리스 스티브와 피오나가 손오공 부르마로 ㅋㅋㅋ      0\n",
       "3                                        0점은 없나요?...      0\n",
       "4                                          제발 시즌2 ㅜㅜ      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_corpus._corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DataLoader` class\n",
    "`torch.utils.data.DataLoader` class의 instance를 생성할 때, `dataset` argument에 위에서 생성한 `Corpus` class의 instance를 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "tr_dl = DataLoader(tr_corpus, shuffle=False, batch_size=128, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # epoch=2, batch=128로 DataLoader (tr_dl)를 순회하는 for문을 작성해보세요\n",
    "# epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    \n",
    "    for mb in tr_dl:\n",
    "        x_mb, y_mb = mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
